{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6LT8uRNYBTM",
        "colab_type": "text"
      },
      "source": [
        "# Lab 4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeDloUTSYNjt",
        "colab_type": "text"
      },
      "source": [
        "## Problem 1: Warm up. Grid Search CV\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsK_S0mgYM6V",
        "colab_type": "code",
        "outputId": "625c8767-d0a3-41b8-e320-34105f4cb759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "print(__doc__)\n",
        "\n",
        "# Loading the Digits dataset\n",
        "digits = datasets.load_digits()\n",
        "digits\n",
        "# To apply an classifier on this data, we need to flatten the image, to\n",
        "# turn the data in a (samples, feature) matrix:\n",
        "n_samples = len(digits.images)\n",
        "X = digits.images.reshape((n_samples, -1))\n",
        "y = digits.target\n",
        "\n",
        "print(\"X:\\n\", X)\n",
        "print(\"Y:\\n,\", y)\n",
        "\n",
        "# Split the dataset in two equal parts\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.5, random_state=0)\n",
        "\n",
        "\n",
        "# Set the parameters by cross-validation\n",
        "# tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
        "#                      'C': [1, 10, 100, 1000]},\n",
        "#                     {'kernel': ['linear'], 'C': [1, 10, 100, 1000]},\n",
        "#                    {'kernel': ['poly'], 'C': [1e-3, 1e-4, 1, 10, 100, 1000]}]\n",
        "tuned_parameters = [{'kernel': ['rbf', 'linear', 'poly'], 'gamma': [1e-3, 1e-4],\n",
        "                     'C': [1, 10, 100, 1000]}]\n",
        "scores = ['precision', 'recall']\n",
        "for score in scores:\n",
        "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
        "    print()\n",
        "\n",
        "    clf = GridSearchCV(SVC(gamma='auto'), tuned_parameters, cv=5,\n",
        "                       scoring='%s_macro' % score)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Best parameters set found on development set:\")\n",
        "    print()\n",
        "    print(clf.best_params_)\n",
        "    print()\n",
        "    print(\"Grid scores on development set:\")\n",
        "    print()\n",
        "    means = clf.cv_results_['mean_test_score']\n",
        "    stds = clf.cv_results_['std_test_score']\n",
        "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
        "              % (mean, std * 2, params))\n",
        "    print()\n",
        "\n",
        "    print(\"Detailed classification report:\")\n",
        "    print()\n",
        "    print(\"The model is trained on the full development set.\")\n",
        "    print(\"The scores are computed on the full evaluation set.\")\n",
        "    print()\n",
        "    y_true, y_pred = y_test, clf.predict(X_test)\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print()\n",
        "\n",
        "# Note the problem is too easy: the hyperparameter plateau is too flat and the\n",
        "# output model is the same for precision and recall with ties in quality."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatically created module for IPython interactive environment\n",
            "X:\n",
            " [[ 0.  0.  5. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ... 10.  0.  0.]\n",
            " [ 0.  0.  0. ... 16.  9.  0.]\n",
            " ...\n",
            " [ 0.  0.  1. ...  6.  0.  0.]\n",
            " [ 0.  0.  2. ... 12.  0.  0.]\n",
            " [ 0.  0. 10. ... 12.  1.  0.]]\n",
            "Y:\n",
            ", [0 1 2 ... 8 9 8]\n",
            "# Tuning hyper-parameters for precision\n",
            "\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "\n",
            "Grid scores on development set:\n",
            "\n",
            "0.986 (+/-0.016) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.975 (+/-0.014) for {'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\n",
            "0.987 (+/-0.020) for {'C': 1, 'gamma': 0.001, 'kernel': 'poly'}\n",
            "0.959 (+/-0.029) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.975 (+/-0.014) for {'C': 1, 'gamma': 0.0001, 'kernel': 'linear'}\n",
            "0.902 (+/-0.040) for {'C': 1, 'gamma': 0.0001, 'kernel': 'poly'}\n",
            "0.988 (+/-0.017) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.975 (+/-0.014) for {'C': 10, 'gamma': 0.001, 'kernel': 'linear'}\n",
            "0.987 (+/-0.020) for {'C': 10, 'gamma': 0.001, 'kernel': 'poly'}\n",
            "0.982 (+/-0.026) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.975 (+/-0.014) for {'C': 10, 'gamma': 0.0001, 'kernel': 'linear'}\n",
            "0.976 (+/-0.020) for {'C': 10, 'gamma': 0.0001, 'kernel': 'poly'}\n",
            "0.988 (+/-0.017) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.975 (+/-0.014) for {'C': 100, 'gamma': 0.001, 'kernel': 'linear'}\n",
            "0.987 (+/-0.020) for {'C': 100, 'gamma': 0.001, 'kernel': 'poly'}\n",
            "0.982 (+/-0.025) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.975 (+/-0.014) for {'C': 100, 'gamma': 0.0001, 'kernel': 'linear'}\n",
            "0.986 (+/-0.019) for {'C': 100, 'gamma': 0.0001, 'kernel': 'poly'}\n",
            "0.988 (+/-0.017) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.975 (+/-0.014) for {'C': 1000, 'gamma': 0.001, 'kernel': 'linear'}\n",
            "0.987 (+/-0.020) for {'C': 1000, 'gamma': 0.001, 'kernel': 'poly'}\n",
            "0.982 (+/-0.025) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.975 (+/-0.014) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'linear'}\n",
            "0.987 (+/-0.020) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'poly'}\n",
            "\n",
            "Detailed classification report:\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        89\n",
            "           1       0.97      1.00      0.98        90\n",
            "           2       0.99      0.98      0.98        92\n",
            "           3       1.00      0.99      0.99        93\n",
            "           4       1.00      1.00      1.00        76\n",
            "           5       0.99      0.98      0.99       108\n",
            "           6       0.99      1.00      0.99        89\n",
            "           7       0.99      1.00      0.99        78\n",
            "           8       1.00      0.98      0.99        92\n",
            "           9       0.99      0.99      0.99        92\n",
            "\n",
            "    accuracy                           0.99       899\n",
            "   macro avg       0.99      0.99      0.99       899\n",
            "weighted avg       0.99      0.99      0.99       899\n",
            "\n",
            "\n",
            "# Tuning hyper-parameters for recall\n",
            "\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'C': 1, 'gamma': 0.001, 'kernel': 'poly'}\n",
            "\n",
            "Grid scores on development set:\n",
            "\n",
            "0.986 (+/-0.019) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.972 (+/-0.012) for {'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\n",
            "0.987 (+/-0.020) for {'C': 1, 'gamma': 0.001, 'kernel': 'poly'}\n",
            "0.957 (+/-0.029) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.972 (+/-0.012) for {'C': 1, 'gamma': 0.0001, 'kernel': 'linear'}\n",
            "0.871 (+/-0.047) for {'C': 1, 'gamma': 0.0001, 'kernel': 'poly'}\n",
            "0.987 (+/-0.019) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.972 (+/-0.012) for {'C': 10, 'gamma': 0.001, 'kernel': 'linear'}\n",
            "0.987 (+/-0.020) for {'C': 10, 'gamma': 0.001, 'kernel': 'poly'}\n",
            "0.981 (+/-0.028) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.972 (+/-0.012) for {'C': 10, 'gamma': 0.0001, 'kernel': 'linear'}\n",
            "0.974 (+/-0.023) for {'C': 10, 'gamma': 0.0001, 'kernel': 'poly'}\n",
            "0.987 (+/-0.019) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.972 (+/-0.012) for {'C': 100, 'gamma': 0.001, 'kernel': 'linear'}\n",
            "0.987 (+/-0.020) for {'C': 100, 'gamma': 0.001, 'kernel': 'poly'}\n",
            "0.981 (+/-0.026) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.972 (+/-0.012) for {'C': 100, 'gamma': 0.0001, 'kernel': 'linear'}\n",
            "0.986 (+/-0.021) for {'C': 100, 'gamma': 0.0001, 'kernel': 'poly'}\n",
            "0.987 (+/-0.019) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.972 (+/-0.012) for {'C': 1000, 'gamma': 0.001, 'kernel': 'linear'}\n",
            "0.987 (+/-0.020) for {'C': 1000, 'gamma': 0.001, 'kernel': 'poly'}\n",
            "0.981 (+/-0.026) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.972 (+/-0.012) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'linear'}\n",
            "0.987 (+/-0.020) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'poly'}\n",
            "\n",
            "Detailed classification report:\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        89\n",
            "           1       0.97      0.99      0.98        90\n",
            "           2       0.99      0.99      0.99        92\n",
            "           3       0.97      0.99      0.98        93\n",
            "           4       1.00      1.00      1.00        76\n",
            "           5       0.96      0.97      0.97       108\n",
            "           6       0.98      0.99      0.98        89\n",
            "           7       0.99      1.00      0.99        78\n",
            "           8       1.00      0.95      0.97        92\n",
            "           9       0.99      0.97      0.98        92\n",
            "\n",
            "    accuracy                           0.98       899\n",
            "   macro avg       0.98      0.98      0.98       899\n",
            "weighted avg       0.98      0.98      0.98       899\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pzj8qHxw2a9",
        "colab_type": "text"
      },
      "source": [
        "## Problem 2: Lasso, Forward Selection and Cross Validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0lDFpy_w5vD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets, linear_model\n",
        "import pandas as pd\n",
        "from pandas import DataFrame, Series\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "sns.set(style='ticks', palette='Set2')\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6UTZZFPAKCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Problem 2.1\n",
        "# generate some sparse data to play with\n",
        "np.random.seed(7)\n",
        "\n",
        "n_samples, n_features = 100, 200\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "\n",
        "k = 5\n",
        "# beta generated with k nonzeros\n",
        "#coef = 10 * np.random.randn(n_features)\n",
        "coef = 10 * np.ones(n_features)\n",
        "inds = np.arange(n_features)\n",
        "print(\"coef\\n\", coef)\n",
        "print(\"inds\\n\", inds)\n",
        "\n",
        "np.random.shuffle(inds)\n",
        "coef[inds[k:]] = 0  # sparsify coef\n",
        "print(\"ind\\n\", inds)\n",
        "print(\"coef after shuffling\\n\", coef)\n",
        "y = np.dot(X, coef).reshape(n_samples, 1)\n",
        "\n",
        "# add noise\n",
        "y += 0.01 * np.random.randn(n_samples, 1)\n",
        "print(\"actual y\\n\", y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7gibpb8BVTQ",
        "colab_type": "code",
        "outputId": "5eb772e5-fce0-41b3-eed2-408c268cb1eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# manually implement forward selection\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "error_list = []\n",
        "r_sq_list = []\n",
        "features_added_idx = []\n",
        "features_added_arr = np.empty((100, 0))\n",
        "\n",
        "# find k non-zero beta values\n",
        "for i in range(0, k):\n",
        "    # find most significant beta out of all the features\n",
        "    for feature in range(0, n_features):\n",
        "      if feature in features_added_idx:\n",
        "        error_list.append(float(\"inf\"))\n",
        "        continue  # skip if feature was already added\n",
        "        \n",
        "      n_samples = X.shape[0]\n",
        "      X_subset = np.append(features_added_arr, X[:, feature].reshape(-1, 1), axis=1)  # get only one feature\n",
        "      \n",
        "      # perform Linear Regression with LSE\n",
        "      model = LinearRegression()\n",
        "      model.fit(X_subset, y)\n",
        "      \n",
        "      # find error and r^2 value\n",
        "      y_pred = model.predict(X_subset)\n",
        "      error = np.sum(np.square(y-y_pred))\n",
        "      # print(f\"error for feature {feature} is {error}\")\n",
        "      error_list.append(error)\n",
        "      \n",
        "      r_sq = model.score(X_subset, y)\n",
        "      # print(f\"r^2 score is {r_sq}\")\n",
        "      r_sq_list.append(r_sq)\n",
        "      \n",
        "      \n",
        "    # add feature with lowest error\n",
        "    min_error = min(error_list)\n",
        "    feature_idx = error_list.index(min_error)\n",
        "    features_added_idx.append(feature_idx)\n",
        "    features_added_arr = np.append(features_added_arr, X[:, feature_idx].reshape(-1, 1), axis=1)\n",
        "    \n",
        "    error_list.clear()\n",
        "    r_sq_list.clear()\n",
        "    \n",
        "print(\"features added in order using forward regression\")    \n",
        "print(features_added_idx)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features added in order using forward regression\n",
            "[18, 15, 51, 78, 34]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v2aXQIeiiso",
        "colab_type": "code",
        "outputId": "33f78f5e-277e-41e0-ccc4-daba1f5ec4a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# Problem 2.3\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Lasso\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Split the dataset in two equal parts\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.5, random_state=0)\n",
        "\n",
        "# Set the parameters by cross-validation\n",
        "tuned_parameters = [{'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]}]\n",
        "scores = ['neg_mean_squared_error', 'r2']\n",
        "for score in scores:\n",
        "    print(f'# Tuning hyper-parameters for {score}')\n",
        "    print()\n",
        "\n",
        "    clf = GridSearchCV(Lasso(), tuned_parameters, cv=5,\n",
        "                       scoring=score)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Best parameters set found on development set:\")\n",
        "    print()\n",
        "    print(clf.best_params_)\n",
        "    print()\n",
        "    print(\"Grid scores on development set:\")\n",
        "    print()\n",
        "    means = clf.cv_results_['mean_test_score']\n",
        "    stds = clf.cv_results_['std_test_score']\n",
        "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "        print(f\"{mean} (+/-{std*2}) for {params}\")\n",
        "        \n",
        "    y_true, y_pred = y_test, clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Tuning hyper-parameters for neg_mean_squared_error\n",
            "\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'alpha': 0.01}\n",
            "\n",
            "Grid scores on development set:\n",
            "\n",
            "-709.6880914478731 (+/-629.7730951190001) for {'alpha': 0.0001}\n",
            "-380.93023150315565 (+/-375.45492098712674) for {'alpha': 0.001}\n",
            "-0.0009572951765671724 (+/-0.0010602438089130483) for {'alpha': 0.01}\n",
            "-0.08016318292552004 (+/-0.09501160006079687) for {'alpha': 0.1}\n",
            "-8.006704642092345 (+/-9.48642981748009) for {'alpha': 1}\n",
            "-478.4405111441862 (+/-498.3533243100588) for {'alpha': 10}\n",
            "-549.2956101694538 (+/-507.0252431973097) for {'alpha': 100}\n",
            "# Tuning hyper-parameters for r2\n",
            "\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'alpha': 0.01}\n",
            "\n",
            "Grid scores on development set:\n",
            "\n",
            "-0.789341662271314 (+/-2.109384712511707) for {'alpha': 0.0001}\n",
            "0.14833073021423782 (+/-0.6651766213136853) for {'alpha': 0.001}\n",
            "0.9999982111012149 (+/-9.592262842603306e-07) for {'alpha': 0.01}\n",
            "0.9998596656294025 (+/-9.7545209422529e-05) for {'alpha': 0.1}\n",
            "0.9859349210885742 (+/-0.009433688658618064) for {'alpha': 1}\n",
            "0.10890350400850124 (+/-0.36241931238783276) for {'alpha': 10}\n",
            "-0.08869053235555988 (+/-0.2097637038151971) for {'alpha': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9psF5c3A_xW",
        "colab_type": "text"
      },
      "source": [
        "We will use the hyperparameter alpha = 0.01."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CqT2GMLidRn",
        "colab_type": "code",
        "outputId": "036a947c-08fd-451b-b76f-3d89ebc698f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Problem 2.4\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Lasso\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Split the dataset in two equal parts\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.5, random_state=0)\n",
        "\n",
        "# Set the parameters by cross-validation\n",
        "tuned_parameters = [{'alpha': [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1]}]\n",
        "folds = [2, 3, 4, 5, 7, 9, 10, 12, 15]\n",
        "# scores = ['r2', 'neg_mean_squared_error']\n",
        "scores = ['r2']\n",
        "for fold in folds:\n",
        "    for score in scores:\n",
        "        print(f'##### Tuning hyper-parameters for {score}')\n",
        "        print()\n",
        "\n",
        "        clf = GridSearchCV(Lasso(), tuned_parameters, cv=fold,\n",
        "                           scoring=score)\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        print(f\"Best parameters set found on development set: {clf.best_params_}, has {fold} folds.\")\n",
        "        print()\n",
        "        print(\"Grid scores on development set:\")\n",
        "        print()\n",
        "        means = clf.cv_results_['mean_test_score']\n",
        "        stds = clf.cv_results_['std_test_score']\n",
        "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "            print(f\"{mean:.4f} (+/-{std*2:.4f}) for {params}\")\n",
        "\n",
        "        y_true, y_pred = y_test, clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##### Tuning hyper-parameters for r2\n",
            "\n",
            "Best parameters set found on development set: {'alpha': 0.1}, has 2 folds.\n",
            "\n",
            "Grid scores on development set:\n",
            "\n",
            "-1.3960 (+/-1.7796) for {'alpha': 1e-05}\n",
            "-1.3959 (+/-1.7795) for {'alpha': 5e-05}\n",
            "-1.3957 (+/-1.7794) for {'alpha': 0.0001}\n",
            "-0.9718 (+/-1.2041) for {'alpha': 0.0005}\n",
            "-0.6798 (+/-0.6721) for {'alpha': 0.001}\n",
            "0.3938 (+/-0.1389) for {'alpha': 0.005}\n",
            "0.6992 (+/-0.2962) for {'alpha': 0.01}\n",
            "0.9956 (+/-0.0086) for {'alpha': 0.05}\n",
            "0.9997 (+/-0.0003) for {'alpha': 0.1}\n",
            "##### Tuning hyper-parameters for r2\n",
            "\n",
            "Best parameters set found on development set: {'alpha': 0.05}, has 3 folds.\n",
            "\n",
            "Grid scores on development set:\n",
            "\n",
            "-0.6647 (+/-1.0177) for {'alpha': 1e-05}\n",
            "-0.6645 (+/-1.0176) for {'alpha': 5e-05}\n",
            "-0.6643 (+/-1.0174) for {'alpha': 0.0001}\n",
            "-0.2307 (+/-0.6170) for {'alpha': 0.0005}\n",
            "-0.0045 (+/-0.5056) for {'alpha': 0.001}\n",
            "0.8438 (+/-0.0464) for {'alpha': 0.005}\n",
            "0.9985 (+/-0.0042) for {'alpha': 0.01}\n",
            "1.0000 (+/-0.0000) for {'alpha': 0.05}\n",
            "0.9998 (+/-0.0001) for {'alpha': 0.1}\n",
            "##### Tuning hyper-parameters for r2\n",
            "\n",
            "Best parameters set found on development set: {'alpha': 0.01}, has 4 folds.\n",
            "\n",
            "Grid scores on development set:\n",
            "\n",
            "-0.7846 (+/-1.6892) for {'alpha': 1e-05}\n",
            "-0.7844 (+/-1.6888) for {'alpha': 5e-05}\n",
            "-0.7841 (+/-1.6885) for {'alpha': 0.0001}\n",
            "-0.4543 (+/-1.3155) for {'alpha': 0.0005}\n",
            "-0.2068 (+/-0.9945) for {'alpha': 0.001}\n",
            "0.9235 (+/-0.1425) for {'alpha': 0.005}\n",
            "1.0000 (+/-0.0000) for {'alpha': 0.01}\n",
            "1.0000 (+/-0.0000) for {'alpha': 0.05}\n",
            "0.9998 (+/-0.0001) for {'alpha': 0.1}\n",
            "##### Tuning hyper-parameters for r2\n",
            "\n",
            "Best parameters set found on development set: {'alpha': 0.01}, has 5 folds.\n",
            "\n",
            "Grid scores on development set:\n",
            "\n",
            "-0.7903 (+/-2.1112) for {'alpha': 1e-05}\n",
            "-0.7899 (+/-2.1104) for {'alpha': 5e-05}\n",
            "-0.7893 (+/-2.1094) for {'alpha': 0.0001}\n",
            "-0.1922 (+/-1.1566) for {'alpha': 0.0005}\n",
            "0.1483 (+/-0.6652) for {'alpha': 0.001}\n",
            "0.9628 (+/-0.0725) for {'alpha': 0.005}\n",
            "1.0000 (+/-0.0000) for {'alpha': 0.01}\n",
            "1.0000 (+/-0.0000) for {'alpha': 0.05}\n",
            "0.9999 (+/-0.0001) for {'alpha': 0.1}\n",
            "##### Tuning hyper-parameters for r2\n",
            "\n",
            "Best parameters set found on development set: {'alpha': 0.01}, has 7 folds.\n",
            "\n",
            "Grid scores on development set:\n",
            "\n",
            "-0.1425 (+/-0.6222) for {'alpha': 1e-05}\n",
            "-0.1423 (+/-0.6223) for {'alpha': 5e-05}\n",
            "-0.1421 (+/-0.6226) for {'alpha': 0.0001}\n",
            "0.1860 (+/-0.7298) for {'alpha': 0.0005}\n",
            "0.3622 (+/-0.5971) for {'alpha': 0.001}\n",
            "0.9974 (+/-0.0105) for {'alpha': 0.005}\n",
            "1.0000 (+/-0.0000) for {'alpha': 0.01}\n",
            "1.0000 (+/-0.0000) for {'alpha': 0.05}\n",
            "0.9999 (+/-0.0001) for {'alpha': 0.1}\n",
            "##### Tuning hyper-parameters for r2\n",
            "\n",
            "Best parameters set found on development set: {'alpha': 0.01}, has 9 folds.\n",
            "\n",
            "Grid scores on development set:\n",
            "\n",
            "-0.7716 (+/-2.3460) for {'alpha': 1e-05}\n",
            "-0.7712 (+/-2.3455) for {'alpha': 5e-05}\n",
            "-0.7707 (+/-2.3449) for {'alpha': 0.0001}\n",
            "-0.1525 (+/-1.5744) for {'alpha': 0.0005}\n",
            "0.1766 (+/-1.0726) for {'alpha': 0.001}\n",
            "0.9976 (+/-0.0116) for {'alpha': 0.005}\n",
            "1.0000 (+/-0.0000) for {'alpha': 0.01}\n",
            "1.0000 (+/-0.0000) for {'alpha': 0.05}\n",
            "0.9999 (+/-0.0001) for {'alpha': 0.1}\n",
            "##### Tuning hyper-parameters for r2\n",
            "\n",
            "Best parameters set found on development set: {'alpha': 0.01}, has 10 folds.\n",
            "\n",
            "Grid scores on development set:\n",
            "\n",
            "-1.1696 (+/-3.3867) for {'alpha': 1e-05}\n",
            "-1.1692 (+/-3.3863) for {'alpha': 5e-05}\n",
            "-1.1686 (+/-3.3857) for {'alpha': 0.0001}\n",
            "-0.3479 (+/-1.8730) for {'alpha': 0.0005}\n",
            "-0.0042 (+/-1.1564) for {'alpha': 0.001}\n",
            "0.9977 (+/-0.0115) for {'alpha': 0.005}\n",
            "1.0000 (+/-0.0000) for {'alpha': 0.01}\n",
            "1.0000 (+/-0.0000) for {'alpha': 0.05}\n",
            "0.9999 (+/-0.0001) for {'alpha': 0.1}\n",
            "##### Tuning hyper-parameters for r2\n",
            "\n",
            "Best parameters set found on development set: {'alpha': 0.01}, has 12 folds.\n",
            "\n",
            "Grid scores on development set:\n",
            "\n",
            "-1.5572 (+/-6.1156) for {'alpha': 1e-05}\n",
            "-1.5565 (+/-6.1136) for {'alpha': 5e-05}\n",
            "-1.5557 (+/-6.1112) for {'alpha': 0.0001}\n",
            "-0.7008 (+/-4.0924) for {'alpha': 0.0005}\n",
            "-0.3340 (+/-3.5263) for {'alpha': 0.001}\n",
            "0.9990 (+/-0.0042) for {'alpha': 0.005}\n",
            "1.0000 (+/-0.0000) for {'alpha': 0.01}\n",
            "1.0000 (+/-0.0000) for {'alpha': 0.05}\n",
            "0.9998 (+/-0.0001) for {'alpha': 0.1}\n",
            "##### Tuning hyper-parameters for r2\n",
            "\n",
            "Best parameters set found on development set: {'alpha': 0.005}, has 15 folds.\n",
            "\n",
            "Grid scores on development set:\n",
            "\n",
            "-1.5541 (+/-4.4766) for {'alpha': 1e-05}\n",
            "-1.5534 (+/-4.4750) for {'alpha': 5e-05}\n",
            "-1.5524 (+/-4.4730) for {'alpha': 0.0001}\n",
            "-0.6391 (+/-3.0453) for {'alpha': 0.0005}\n",
            "-0.3153 (+/-3.5884) for {'alpha': 0.001}\n",
            "1.0000 (+/-0.0000) for {'alpha': 0.005}\n",
            "1.0000 (+/-0.0000) for {'alpha': 0.01}\n",
            "0.9999 (+/-0.0003) for {'alpha': 0.05}\n",
            "0.9997 (+/-0.0013) for {'alpha': 0.1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TkOXkfiDVRf",
        "colab_type": "text"
      },
      "source": [
        "The value of the hyperparameter varies. It seems that alpha goes down, generally, as there are more folds. This is probably due to overfitting on the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFbCyhlUE_V7",
        "colab_type": "code",
        "outputId": "3d1ffcd3-8d0d-4e89-d37b-c2cfec17a2a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Problem 2.5\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Lasso, LassoCV\n",
        "\n",
        "alphas = [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1]\n",
        "folds = [2, 3, 4, 5, 7, 9, 10, 12, 15]\n",
        "\n",
        "for fold in folds:\n",
        "    lasso_cv = LassoCV(alphas=alphas, cv=fold).fit(X, y)\n",
        "    print(f'Lasso CV has R^2 of {lasso_cv.score(X, y):.7f}. Alpha is: {lasso_cv.alpha_}. # folds are: {fold}.')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lasso CV has R^2 of 1.0000000. Alpha is: 1e-05. # folds are: 2.\n",
            "Lasso CV has R^2 of 0.9999996. Alpha is: 0.001. # folds are: 3.\n",
            "Lasso CV has R^2 of 0.9999999. Alpha is: 0.0005. # folds are: 4.\n",
            "Lasso CV has R^2 of 0.9999999. Alpha is: 0.0005. # folds are: 5.\n",
            "Lasso CV has R^2 of 0.9999999. Alpha is: 0.0005. # folds are: 7.\n",
            "Lasso CV has R^2 of 0.9999996. Alpha is: 0.001. # folds are: 9.\n",
            "Lasso CV has R^2 of 0.9999996. Alpha is: 0.001. # folds are: 10.\n",
            "Lasso CV has R^2 of 0.9999999. Alpha is: 0.0005. # folds are: 12.\n",
            "Lasso CV has R^2 of 0.9999996. Alpha is: 0.001. # folds are: 15.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toTns9bB3K9O",
        "colab_type": "text"
      },
      "source": [
        "Gotta ask about this one. Actually don't have a clue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_K_O2tOGw_m",
        "colab_type": "code",
        "outputId": "87c64d47-d0e1-41ad-872a-1b1696f3c997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Problem 3\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# generate some sparse data to play with\n",
        "np.random.seed(7)\n",
        "\n",
        "n_samples, n_features = 100, 200\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "\n",
        "k = 5\n",
        "# beta generated with k nonzeros\n",
        "#coef = 10 * np.random.randn(n_features)\n",
        "coef = 10 * np.ones(n_features)\n",
        "inds = np.arange(n_features)\n",
        "# print(\"coef\\n\", coef)\n",
        "# print(\"inds\\n\", inds)\n",
        "\n",
        "np.random.shuffle(inds)\n",
        "coef[inds[k:]] = 0  # sparsify coef\n",
        "# print(\"ind\\n\", inds)\n",
        "# print(\"coef after shuffling\\n\", coef)\n",
        "y = np.dot(X, coef).reshape(n_samples, 1)\n",
        "\n",
        "# add noise\n",
        "y += 0.01 * np.random.randn(n_samples, 1)\n",
        "# print(\"actual y\\n\", y)\n",
        "\n",
        "\n",
        "# implement k-fold cross-validation\n",
        "# split the data up into k subsets\n",
        "k_values = [1.2, 1.5, 2, 5, 10, 50, 100]\n",
        "num_of_trials = 50\n",
        "beta_star = coef\n",
        "nonzero_betas = [15, 18, 34, 51, 78]\n",
        "betas = np.empty((200, 0))\n",
        "\n",
        "\n",
        "for k in k_values:\n",
        "  for i in range(num_of_trials):\n",
        "    # regenerate y to get different noise\n",
        "    y = np.dot(X, coef).reshape(n_samples, 1)\n",
        "    # add noise\n",
        "    y += 0.01 * np.random.randn(n_samples, 1)\n",
        "    \n",
        "    testSize = 1/k\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testSize)\n",
        "\n",
        "    lasso = Lasso()\n",
        "    lasso.fit(X_train, y_train)\n",
        "    train_score = lasso.score(X_train, y_train)\n",
        "    test_score = lasso.score(X_test, y_test)\n",
        "    coeff_used = np.sum(lasso.coef_!=0)\n",
        "    \n",
        "    betas = np.append(betas, lasso.coef_.reshape(200, 1), axis=1)\n",
        "    \n",
        "  # calculate bias  \n",
        "  beta_means = np.mean(betas, axis=1)\n",
        "  bias = beta_star.reshape(200) - beta_means\n",
        "  total_bias = np.sum(np.square(bias))\n",
        "  \n",
        "  # calculate variance\n",
        "  beta_var = np.var(betas, axis=1)\n",
        "  avg_var = np.sum(beta_var)/200\n",
        "  # print(betas, \"\\n\\n\")\n",
        "  \n",
        "  print(f\"For k = {k}\")\n",
        "  for beta in nonzero_betas:\n",
        "    print(f\"for feature {k}, bias = {bias[beta]}, variance = {beta_var[beta]}\")  \n",
        "  print(f\"total bias is {total_bias}\")\n",
        "  print(f\"average variance = {avg_var}\\n\")\n",
        "  \n",
        "  # print(errors.shape)\n",
        "  # print(f\"calculated bias is {np.mean(errors, axis=1)}\")\n",
        "  \n",
        "  \n",
        "  betas = np.empty((200, 0))\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For k = 1.2\n",
            "for feature 1.2, bias = 8.466428248155001, variance = 4.71841880886136\n",
            "for feature 1.2, bias = 7.919714858757581, variance = 10.275729524512737\n",
            "for feature 1.2, bias = 8.72803310316235, variance = 5.629021776035108\n",
            "for feature 1.2, bias = 8.477316147391047, variance = 8.959620511238011\n",
            "for feature 1.2, bias = 8.910355885810182, variance = 5.823641417628076\n",
            "total bias is 365.3960188057588\n",
            "average variance = 0.5712874172860043\n",
            "\n",
            "(0, 0)\n",
            "For k = 1.5\n",
            "for feature 1.5, bias = 3.2826642178462535, variance = 6.80632492468971\n",
            "for feature 1.5, bias = 2.7332342638012843, variance = 6.311095501238107\n",
            "for feature 1.5, bias = 3.781941580252318, variance = 8.644463919989093\n",
            "for feature 1.5, bias = 3.30716152351148, variance = 7.717223129665692\n",
            "for feature 1.5, bias = 3.2663474376082977, variance = 8.127489499673894\n",
            "total bias is 54.75064613404826\n",
            "average variance = 0.24383361277874838\n",
            "\n",
            "(200, 0)\n",
            "For k = 2\n",
            "for feature 2, bias = 1.255560283188622, variance = 0.10144424487246839\n",
            "for feature 2, bias = 1.0579020794311091, variance = 0.12851887094323497\n",
            "for feature 2, bias = 1.5726015922104803, variance = 0.30928111765482463\n",
            "for feature 2, bias = 1.2682637147538536, variance = 0.18352744726836676\n",
            "for feature 2, bias = 1.3584999384731162, variance = 0.09054941916507403\n",
            "total bias is 8.623889300103425\n",
            "average variance = 0.004233042031336052\n",
            "\n",
            "(200, 0)\n",
            "For k = 5\n",
            "for feature 5, bias = 1.2115206974809496, variance = 0.018071255297186537\n",
            "for feature 5, bias = 1.0327063955493756, variance = 0.016204909389270286\n",
            "for feature 5, bias = 1.4940734927111539, variance = 0.027618727561037074\n",
            "for feature 5, bias = 1.242310379400175, variance = 0.028816744579392192\n",
            "for feature 5, bias = 1.3433140570943394, variance = 0.016528373407992823\n",
            "total bias is 8.114348236208077\n",
            "average variance = 0.0005362000511743946\n",
            "\n",
            "(200, 0)\n",
            "For k = 10\n",
            "for feature 10, bias = 1.169995086381041, variance = 0.006349882565027829\n",
            "for feature 10, bias = 1.0130081665156858, variance = 0.007693597639321921\n",
            "for feature 10, bias = 1.3996504964927663, variance = 0.008342052242417382\n",
            "for feature 10, bias = 1.1937250826402206, variance = 0.011790494678657958\n",
            "for feature 10, bias = 1.3166606049325065, variance = 0.00717575347717703\n",
            "total bias is 7.512670281421334\n",
            "average variance = 0.0002067589030130106\n",
            "\n",
            "(200, 0)\n",
            "For k = 50\n",
            "for feature 50, bias = 1.1965406857348668, variance = 0.0025646129728523126\n",
            "for feature 50, bias = 1.0231993382851403, variance = 0.0044007042483727625\n",
            "for feature 50, bias = 1.4150691539497426, variance = 0.003239945000338992\n",
            "for feature 50, bias = 1.229009260690633, variance = 0.005385077957623454\n",
            "for feature 50, bias = 1.334053226360231, variance = 0.003393315953366318\n",
            "total bias is 7.771228982571532\n",
            "average variance = 9.491828066276919e-05\n",
            "\n",
            "(200, 0)\n",
            "For k = 100\n",
            "for feature 100, bias = 1.1804922564656835, variance = 0.0006994621765164201\n",
            "for feature 100, bias = 1.0156674441645297, variance = 0.0012013913878234934\n",
            "for feature 100, bias = 1.4045212835403227, variance = 0.0010577454939685667\n",
            "for feature 100, bias = 1.213387238292018, variance = 0.002217414637804803\n",
            "for feature 100, bias = 1.31873067400163, variance = 0.0013792853668052418\n",
            "total bias is 7.609181541231629\n",
            "average variance = 3.2776495314592624e-05\n",
            "\n",
            "(200, 0)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}